#    单机单GPU配置文件
#    该文件包含了训练和评估模型的配置参数。

# 计算设备设置：使用GPU进行训练，若没有GPU可改为"cpu"
device = "cuda"

# 原始数据集路径
data-root = "../data/flowerclassify/train/train"

# 标签文件路径
data-label = "../data/flowerclassify/train_labels.csv"

# 数据集划分比例: 验证集和测试集的比例
valid-split-ratio = 0.15
test-split-ratio = 0.15

# 测试集配置
# 测试集CSV文件路径
test-csv-file = "datasets/test_split.csv"
# 测试集图像目录路径
test-img-dir = "datasets/test"
# 自定义模型参数路径：直接指定完整的模型参数文件路径
custom-model-params-path = "checkpoints/OneGPU/20251025_071825/best-ckpt.pt"
# 自定义输出路径：指定评估结果保存的目录
custom-output-path = "checkpoints/OneGPU/20251025_071825/predictions.csv"


# 模型名称：指定要使用的模型架构
# 可选值："resnet18", "resnet34", "resnet50"
model-name = "resnet50"

# 批次大小：每次训练迭代使用的样本数量，影响训练速度和内存占用
batch-size = 128  # 根据GPU内存情况可适当调整

# 训练轮次：整个数据集训练的次数
num-epochs = 100

# 数据加载线程数：用于并行加载数据的线程数量，可根据CPU核心数调整
num-workers = 20

# 类别数量：花卉分类任务中的类别总数
num-classes = 0

# 日志打印间隔：每训练多少个批次打印一次日志
log-interval = 10

# 是否加载检查点：是否从之前保存的模型继续训练
load-checkpoint = false

# 是否使用预训练模型：是否使用在大规模数据集上预训练的权重初始化模型
load-pretrained = true

# 加载检查点路径：预训练模型或之前保存的检查点路径
load-checkpoint-path = "checkpoints/best-ckpt.pt"

# 最佳模型保存路径：训练过程中性能最好的模型将保存在此路径
#best-checkpoint-path = "checkpoints/best-ckpt.pt"

# 最新模型保存路径：每个epoch结束后，将当前模型保存在此路径
#last-checkpoint-path = "checkpoints/last-ckpt.pt"

# 损失函数类型：指定训练使用的损失函数
# 可选值："cross_entropy", "nll_loss", "bce_with_logits", "l1_regularized_cross_entropy"
loss-function = "l1_regularized_cross_entropy"  # 切换为 l1_regularized_cross_entropy 启用L1正则化

# 学习率：控制模型参数更新的步长，较小的值有助于稳定训练
learning-rate = 0.00005

# 权重衰减：L2正则化系数，用于防止过拟合
weight-decay = 0.0001

# 优化器类型：指定训练使用的优化器
# 可选值："adam", "adamw", "sgd", "rmsprop"
optimizer-type = "adam"

# 学习率调度器类型：指定训练使用的学习率调度器类型
# 可选值："step", "multi_step", "exponential", "cosine", "cosine_warm_restarts"
# 学习率调度器 相关参数去 optim_utils.py 中定义，根据实际情况调整
lr-scheduler-type = "cosine_warm_restarts"

# 学习率调度器步长：每隔多少个epoch调整一次学习率
lr-scheduler-step-size = 10

# 学习率调度器衰减率：学习率调整的比例
lr-scheduler-gamma = 0.5

warmup_epochs = 5         # 预热轮数
warmup_type = "cosine"     # 预热类型：linear或cosine

# 早停机制耐心值：连续多少个epoch无性能提升后停止训练
early-stopping-patience = 10

# 正则化相关配置 0则代表不使用L1正则化
#将l1-lambda从0.0001改为更小的值
l1-lambda = 0.0000005

use-layer-norm = true       # 是否使用LayerNorm层
use-grad-clip = true        # 是否使用梯度裁剪
grad-clip-value = 1.0       # 梯度裁剪阈值

# 激活函数配置，可选值：'gelu', 'swish', 'mish', 'leaky_relu', 'relu'
activation-fn = 'gelu'